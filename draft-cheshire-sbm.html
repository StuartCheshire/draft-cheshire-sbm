<!DOCTYPE html>
<html lang="en" class="Internet-Draft">
<head>
<meta charset="utf-8">
<meta content="Common,Latin" name="scripts">
<meta content="initial-scale=1.0" name="viewport">
<title>Source Buffer Management</title>
<meta content="Stuart Cheshire" name="author">
<meta content="
       In the past decade there has been growing awareness about the
harmful effects of bufferbloat in the network, and there has
been good work on developments like L4S to address that problem.
However, bufferbloat on the sender itself remains a significant
additional problem, which has not received similar attention.
This document offers techniques and guidance for host networking
software to avoid network traffic suffering unnecessary delays
caused by excessive buffering at the sender. These improvements
are broadly applicable across all datagram and transport
protocols (UDP, TCP, QUIC, etc.) on all operating systems. 
    " name="description">
<meta content="xml2rfc 3.28.0" name="generator">
<meta content="Bufferbloat" name="keyword">
<meta content="Latency" name="keyword">
<meta content="Responsiveness" name="keyword">
<meta content="draft-cheshire-sbm-latest" name="ietf.draft">
<!-- Generator version information:
  xml2rfc 3.28.0
    Python 3.12.9
    ConfigArgParse 1.7
    google-i18n-address 3.1.1
    intervaltree 3.1.0
    Jinja2 3.1.5
    lxml 5.3.0
    platformdirs 4.3.6
    pycountry 24.6.1
    PyYAML 6.0.2
    requests 2.32.3
    setuptools 70.3.0
    wcwidth 0.2.13
-->
<link href="draft-cheshire-sbm.xml" rel="alternate" type="application/rfc+xml">
<link href="#copyright" rel="license">
<style type="text/css">@font-face {
  font-family: 'Lora';
  font-style: italic;
  font-weight: 400;
  font-display: swap;
  src: local('Lora Italic'), local('Lora-Italic'), url('https://martinthomson.github.io/rfc-css/fonts/lora-italic-cyrillic-ext.woff2') format('woff2');
  unicode-range: U+0460-052F, U+1C80-1C88, U+20B4, U+2DE0-2DFF, U+A640-A69F, U+FE2E-FE2F;
}
@font-face {
  font-family: 'Lora';
  font-style: italic;
  font-weight: 400;
  font-display: swap;
  src: local('Lora Italic'), local('Lora-Italic'), url('https://martinthomson.github.io/rfc-css/fonts/lora-italic-cyrillic-ext.woff2') format('woff2');
  unicode-range: U+0400-045F, U+0490-0491, U+04B0-04B1, U+2116;
}
@font-face {
  font-family: 'Lora';
  font-style: italic;
  font-weight: 400;
  font-display: swap;
  src: local('Lora Italic'), local('Lora-Italic'), url('https://martinthomson.github.io/rfc-css/fonts/lora-italic-vietnamese.woff2') format('woff2');
  unicode-range: U+0102-0103, U+0110-0111, U+1EA0-1EF9, U+20AB;
}
@font-face {
  font-family: 'Lora';
  font-style: italic;
  font-weight: 400;
  font-display: swap;
  src: local('Lora Italic'), local('Lora-Italic'), url('https://martinthomson.github.io/rfc-css/fonts/lora-italic-latin-ext.woff2') format('woff2');
  unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}

@font-face {
  font-family: 'Lora';
  font-style: italic;
  font-weight: 400;
  font-display: swap;
  src: local('Lora Italic'), local('Lora-Italic'), url('https://martinthomson.github.io/rfc-css/fonts/lora-italic-latin.woff2') format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
@font-face {
  font-family: 'Lora';
  font-style: normal;
  font-weight: 400;
  font-display: swap;
  src: local('Lora Regular'), local('Lora-Regular'), url('https://martinthomson.github.io/rfc-css/fonts/lora-regular-cyrillic-ext.woff2') format('woff2');
  unicode-range: U+0460-052F, U+1C80-1C88, U+20B4, U+2DE0-2DFF, U+A640-A69F, U+FE2E-FE2F;
}
@font-face {
  font-family: 'Lora';
  font-style: normal;
  font-weight: 400;
  font-display: swap;
  src: local('Lora Regular'), local('Lora-Regular'), url('https://martinthomson.github.io/rfc-css/fonts/lora-regular-cyrillic.woff2') format('woff2');
  unicode-range: U+0400-045F, U+0490-0491, U+04B0-04B1, U+2116;
}
@font-face {
  font-family: 'Lora';
  font-style: normal;
  font-weight: 400;
  font-display: swap;
  src: local('Lora Regular'), local('Lora-Regular'), url('https://martinthomson.github.io/rfc-css/fonts/lora-regular-vietnamese.woff2') format('woff2');
  unicode-range: U+0102-0103, U+0110-0111, U+1EA0-1EF9, U+20AB;
}
@font-face {
  font-family: 'Lora';
  font-style: normal;
  font-weight: 400;
  font-display: swap;
  src: local('Lora Regular'), local('Lora-Regular'), url('https://martinthomson.github.io/rfc-css/fonts/lora-regular-latin-ext.woff2') format('woff2');
  unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
@font-face {
  font-family: 'Lora';
  font-style: normal;
  font-weight: 400;
  font-display: swap;
  src: local('Lora Regular'), local('Lora-Regular'), url('https://martinthomson.github.io/rfc-css/fonts/lora-regular-latin.woff2') format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}

@font-face {
  font-family: 'Lora';
  font-style: normal;
  font-weight: 700;
  font-display: swap;
  src: local('Lora Bold'), local('Lora-Bold'), url('https://martinthomson.github.io/rfc-css/fonts/lora-bold-cyrillic-ext.woff2') format('woff2');
  unicode-range: U+0460-052F, U+1C80-1C88, U+20B4, U+2DE0-2DFF, U+A640-A69F, U+FE2E-FE2F;
}
@font-face {
  font-family: 'Lora';
  font-style: normal;
  font-weight: 700;
  font-display: swap;
  src: local('Lora Bold'), local('Lora-Bold'), url('https://martinthomson.github.io/rfc-css/fonts/lora-bold-cyrillic.woff2') format('woff2');
  unicode-range: U+0400-045F, U+0490-0491, U+04B0-04B1, U+2116;
}
@font-face {
  font-family: 'Lora';
  font-style: normal;
  font-weight: 700;
  font-display: swap;
  src: local('Lora Bold'), local('Lora-Bold'), url('https://martinthomson.github.io/rfc-css/fonts/lora-bold-vietnamese.woff2') format('woff2');
  unicode-range: U+0102-0103, U+0110-0111, U+1EA0-1EF9, U+20AB;
}
@font-face {
  font-family: 'Lora';
  font-style: normal;
  font-weight: 700;
  font-display: swap;
  src: local('Lora Bold'), local('Lora-Bold'), url('https://martinthomson.github.io/rfc-css/fonts/lora-bold-latin-ext.woff2') format('woff2');
  unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
@font-face {
  font-family: 'Lora';
  font-style: normal;
  font-weight: 700;
  font-display: swap;
  src: local('Lora Bold'), local('Lora-Bold'), url('https://martinthomson.github.io/rfc-css/fonts/lora-bold-latin.woff2') format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
@font-face {
  font-family: 'Lora';
  font-style: normal;
  font-weight: 600;
  font-display: swap;
  src: local('Lora SemiBold'), local('Lora-SemiBold'), url('https://martinthomson.github.io/rfc-css/fonts/lora-semibold-latin.woff2') format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}

@font-face {
  font-family: 'Oxygen Mono';
  font-style: normal;
  font-weight: 400;
  font-display: swap;
  src: local('Oxygen Mono'), local('OxygenMono-Regular'), url('https://martinthomson.github.io/rfc-css/fonts/oxygenmono-regular-latin-ext.woff2') format('woff2');
  unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
@font-face {
  font-family: 'Oxygen Mono';
  font-style: normal;
  font-weight: 400;
  font-display: swap;
  src: local('Oxygen Mono'), local('OxygenMono-Regular'), url('https://martinthomson.github.io/rfc-css/fonts/oxygenmono-regular-latin.woff2') format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}

@font-face {
  font-family: 'Sofia Sans Semi Condensed';
  font-style: italic;
  font-weight: 1 1000;
  src: url('https://martinthomson.github.io/rfc-css/fonts/sofiasanssemicondensed-italic-cyrillic-ext.woff2') format('woff2');
  unicode-range: U+0460-052F, U+1C80-1C8A, U+20B4, U+2DE0-2DFF, U+A640-A69F, U+FE2E-FE2F;
}
@font-face {
  font-family: 'Sofia Sans Semi Condensed';
  font-style: italic;
  font-weight: 1 1000;
  src: url('https://martinthomson.github.io/rfc-css/fonts/sofiasanssemicondensed-italic-cyrillic.woff2') format('woff2');
  unicode-range: U+0301, U+0400-045F, U+0490-0491, U+04B0-04B1, U+2116;
}
@font-face {
  font-family: 'Sofia Sans Semi Condensed';
  font-style: italic;
  font-weight: 1 1000;
  src: url('https://martinthomson.github.io/rfc-css/fonts/sofiasanssemicondensed-italic-greek.woff2') format('woff2');
  unicode-range: U+0370-0377, U+037A-037F, U+0384-038A, U+038C, U+038E-03A1, U+03A3-03FF;
}
@font-face {
  font-family: 'Sofia Sans Semi Condensed';
  font-style: italic;
  font-weight: 1 1000;
  src: url('https://martinthomson.github.io/rfc-css/fonts/sofiasanssemicondensed-italic-latin-ext.woff2') format('woff2');
  unicode-range: U+0100-02BA, U+02BD-02C5, U+02C7-02CC, U+02CE-02D7, U+02DD-02FF, U+0304, U+0308, U+0329, U+1D00-1DBF, U+1E00-1E9F, U+1EF2-1EFF, U+2020, U+20A0-20AB, U+20AD-20C0, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
@font-face {
  font-family: 'Sofia Sans Semi Condensed';
  font-style: italic;
  font-weight: 1 1000;
  src: url('https://martinthomson.github.io/rfc-css/fonts/sofiasanssemicondensed-italic-latin.woff2') format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+0304, U+0308, U+0329, U+2000-206F, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
@font-face {
  font-family: 'Sofia Sans Semi Condensed';
  font-style: normal;
  font-weight: 1 1000;
  src: url('https://martinthomson.github.io/rfc-css/fonts/sofiasanssemicondensed-regular-cyrillic-ext.woff2') format('woff2');
  unicode-range: U+0460-052F, U+1C80-1C8A, U+20B4, U+2DE0-2DFF, U+A640-A69F, U+FE2E-FE2F;
}
@font-face {
  font-family: 'Sofia Sans Semi Condensed';
  font-style: normal;
  font-weight: 1 1000;
  src: url('https://martinthomson.github.io/rfc-css/fonts/sofiasanssemicondensed-regular-cyrillic.woff2') format('woff2');
  unicode-range: U+0301, U+0400-045F, U+0490-0491, U+04B0-04B1, U+2116;
}
@font-face {
  font-family: 'Sofia Sans Semi Condensed';
  font-style: normal;
  font-weight: 1 1000;
  src: url('https://martinthomson.github.io/rfc-css/fonts/sofiasanssemicondensed-regular-greek.woff2') format('woff2');
  unicode-range: U+0370-0377, U+037A-037F, U+0384-038A, U+038C, U+038E-03A1, U+03A3-03FF;
}
@font-face {
  font-family: 'Sofia Sans Semi Condensed';
  font-style: normal;
  font-weight: 1 1000;
  src: url('https://martinthomson.github.io/rfc-css/fonts/sofiasanssemicondensed-regular-latin-ext.woff2') format('woff2');
  unicode-range: U+0100-02BA, U+02BD-02C5, U+02C7-02CC, U+02CE-02D7, U+02DD-02FF, U+0304, U+0308, U+0329, U+1D00-1DBF, U+1E00-1E9F, U+1EF2-1EFF, U+2020, U+20A0-20AB, U+20AD-20C0, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
@font-face {
  font-family: 'Sofia Sans Semi Condensed';
  font-style: normal;
  font-weight: 1 1000;
  src: url('https://martinthomson.github.io/rfc-css/fonts/sofiasanssemicondensed-regular-latin.woff2') format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+0304, U+0308, U+0329, U+2000-206F, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}

:root {
  color-scheme: light dark;
  --background-color: #fff;
  --text-color: #222;
  --title-color: #191919;
  --link-color: #2a6496;
  --highlight-color: #f9f9f9;
  --line-color: #eee;
  --pilcrow-weak: #ddd;
  --pilcrow-strong: #bbb;
  --small-font-size: 14.5px;
  --font-mono: 'Oxygen Mono', monospace;
  --font-title: "Sofia Sans Semi Condensed", sans-serif;
  scrollbar-color: #bbb #eee;
}
body {
  max-width: 600px;
  margin: 75px auto;
  padding: 0 5px;
  color: var(--text-color);
  background-color: var(--background-color);
  font: 16px/22px "Lora", serif;
  scroll-behavior: smooth;
}

.ears {
  display: none;
}

/* headings */
section {
  clear: both;
}
.section-number {
  padding-right: 0.5em;
}
h1, h2, h3, h4, h5, h6 {
  font-family: var(--font-title);
  font-weight: 680;
  margin: 0.8em 0 0.3em;
  font-size-adjust: 0.5;
  color: var(--title-color);
}
h1#title {
  font-size: 32px;
  line-height: 40px;
  clear: both;
}
h1#title, h1#rfcnum {
  margin: 1.5em 0 0.2em;
}
h1#rfcnum + h1#title {
  margin: 0.2em 0;
}

h1, h2, h3 {
  font-size: 22px;
  line-height: 27px;
}
h4, h5, h6 {
  font-size: 20px;
  line-height: 24px;
}

/* general structure */
.author {
  padding-bottom: 0.3em;
  vertical-align: top;
}
#abstract+p {
  font-size: 18px;
  line-height: 24px;
}
#abstract+p code, #abstract+p samp, #abstract+p tt {
  font-size: 16px;
  line-height: 0;
}

p {
  padding: 0;
  margin: 0.5em 0;
  text-align: left;
}
div {
  margin: 0;
}
.alignRight.art-text {
  background-color: var(--highlight-color);
  border: 1px solid var(--line-color);
  border-radius: 3px;
  padding: 0.5em 1em 0;
  margin-bottom: 0.5em;
}
.alignRight.art-text pre {
  padding: 0;
  width: auto;
}
.alignRight {
  margin: 1em 0;
}
.alignRight > *:first-child {
  border: none;
  margin: 0;
  float: right;
  clear: both;
}
.alignRight > *:nth-child(2) {
  clear: both;
  display: block;
  border: none;
}
svg {
  display: block;
}
/* font-family isn't space-separated, but =~ will have to do */
svg[font-family~="monospace" i], svg [font-family~="monospace" i] {
  font-family: var(--font-mono);
}
.alignCenter.art-text {
  background-color: var(--highlight-color);
  border: 1px solid var(--line-color);
  border-radius: 3px;
  padding: 0.5em 1em 0;
  margin-bottom: 0.5em;
}
.alignCenter.art-text pre {
  padding: 0;
  width: auto;
}
.alignCenter {
  margin: 1em 0;
}
.alignCenter > *:first-child {
  border: none;
  /* this isn't optimal, but it's an existence proof.  PrinceXML doesn't
     support flexbox yet.
  */
  display: table;
  margin: 0 auto;
}

/* lists */
ol, ul {
  padding: 0;
  margin: 0 0 0.5em 2em;
  & :is(ol, ul) {
    margin-left: 1em;
  }
}
li {
  margin: 0 0 0.25em 0;
}
ul.empty, .ulEmpty {
  list-style-type: none;
  & li {
    margin-top: 0.5em;
  }
}
:is(ul, ol).compact, .ulCompact, .olCompact {
  margin: 0 0 0 2em;
  & li {
    margin: 0;
    & :first-child { margin-top: 0; }
    & :last-child { margin-bottom: 0; }
  }
}

/* definition lists */
dl {
  clear: left;
  --indent: 3ch;
  /* --indent: attr(indent ch); not supported in any browser, but we can dream */
  &.olPercent {
    --indent: 5ch;
    & > dt {
      min-width: calc(var(--indent) - 2ch);
    }
  }
  &.olPercent > dt {
    float: none;
  }

  dl > dd > & {
    margin-top: 0.5em;
    margin-bottom: 0;
  }
}
dl:not(.dlNewline) > dt {
  float: left;
  margin-right: 2ch;
  min-width: 8ch;
}
dl > dd {
  margin-bottom: .8em;
  margin-left: var(--indent) !important; /* stupid element overrides */
  min-height: 2ex;
}
:is(dl.compact, .dlCompact) > dd {
  margin-bottom: 0;
  & > :is(:first-child, .break:first-child + *) {
    margin-top: 0;
  }
  & > :is(:last-child) {
    margin-bottom: 0;
  }
}
:is(dd, span).break {
  display: none;
}

/* links */
a, a[href].selfRef:hover {
  text-decoration: none;
}
a[href] {
  color: var(--link-color);
}
a[href].selfRef, .iref + a[href].internal {
  color: var(--text-color);
}
a[href]:hover {
  text-decoration: underline;
}
a[href].selfRef:hover {
  background-color: var(--highlight-color);
}
a.xref:is(.cite, .auto), :is(#status-of-memo, #copyright) a {
  white-space: nowrap;
}

/* Figures */
tt, code, pre {
  background-color: var(--highlight-color);
  font: 14px/22px var(--font-mono);
}
tt, code {
  /* changing the font for inline elements leads to different ascender
     and descender heights; as we want to retain baseline alignment,
     remove leading to avoid altering the final height of lines
     note: this fails if these blocks take an entire line,
     a different solution would be great */
  line-height: 0;
}
:is(h1, h2, h3, h4, h5, h6) :is(tt, code) {
  font-size: 84%;
}
pre {
  border: 1px solid var(--line-color);
  font-size: 13.5px;
  line-height: 16px;
  letter-spacing: -0.2px;
  margin: 5px;
  padding: 5px;
}
img {
  max-width: 100%;
}
figure {
  margin: 0.5em 0;
  padding: 0;
}
figure blockquote {
  margin: 0.8em 0.4em 0.4em;
}
figcaption, caption {
  font-style: italic;
  margin: 0.5em 1.5em;
  text-align: left;
  caption-side: bottom;
}
@media screen {
  /* Auto-collapse boilerplate. */
  :is(#status-of-memo, #copyright) p {
    margin: -2px 0;
    max-height: 0;
    transition: max-height 2s ease, margin 0.5s ease 0.5s;
    overflow: hidden;
  }
  :is(#status-of-memo, #copyright):hover p,
  :is(#status-of-memo, #copyright) h2:target ~ p {
    margin: 0.5em 0;
    max-height: 500px;
    overflow: auto;
  }
  pre, svg {
    display: inline-block;
    /* In the horizontal direction, sometimes people make over-sized figures.
       Scrollbars for those is therefore necessary: auto adds them as necessary..
       In the vertical direction, the line-height can combine with the font
       asender/descender height to produce scrollbars: hidden avoids that. */
    overflow: auto hidden;
  }
  pre {
    max-width: 100%;
    width: calc(100% - 22px - 1em);
  }
  svg {
    max-width: calc(100% - 22px - 1em);
  }
  figure pre {
    display: block;
    width: calc(100% - 25px);
  }
  :is(pre, svg) + .pilcrow {
    display: inline-block;
    vertical-align: text-bottom;
    padding-bottom: 8px;
  }
}

/* aside, blockquote */
aside, blockquote {
  margin-left: 0;
  padding: 0 2em;
  font-style: italic;
}
blockquote {
  margin: 1em 0;
}
cite {
  display: block;
  text-align: right;
  font-style: italic;
}

/* tables */
table {
  width: auto;
  max-width: 100%;
  margin: 0 0 1em;
  border-collapse: collapse;
}
table.right {
  margin-left: auto;
}
table.center {
  margin-left: auto;
  margin-right: auto;
}
table.left {
  margin-right: auto;
}
table .text-left {
  text-align: left;
}
table .text-center {
  text-align: center;
}
table .text-right {
  text-align: right;
}

thead, tbody {
  border: 1px solid var(--line-color);
}
th, td {
  text-align: left;
  vertical-align: top;
  padding: 5px 10px;
}
th {
  background-color: var(--line-color);
}
:is(tr:nth-child(2n), thead+tbody > tr:nth-child(2n+1)) > td {
  background-color: var(--background-color);
}
:is(tr:nth-child(2n+1), thead+tbody > tr:nth-child(2n)) > td {
  background-color: var(--highlight-color);
}
table caption {
  margin: 0;
  padding: 3px 0 3px 1em;
}
table p {
  margin: 0;
}

/* pilcrow */
a.pilcrow {
  margin-left: 3px;
  opacity: 0.2;
  user-select: none;
  &[href] {
    color: var(--pilcrow-weak);
    &:hover { text-decoration: none; }
  }
}
@media not print {
  :hover > a.pilcrow {
    opacity: 1;
  }
  a.pilcrow[href]:hover {
    color: var(--pilcrow-strong);
    background-color: transparent;
  }
}
@media print {
  a.pilcrow {
    display: none;
  }
}

/* misc */
hr {
  border: 0;
  border-top: 1px solid var(--line-color);
}
.bcp14 {
  font-variant: small-caps;
  font-weight: 600;
  font-size: var(--small-font-size);
}
.role {
  font-variant: all-small-caps;
}
sub, sup {
  line-height: 1;
  font-size: 80%;
}

/* info block */
#identifiers {
  margin: 0;
  font-size: var(--small-font-size);
  line-height: 18px;
  --identifier-width: 15ch;
  & dt {
    width: var(--identifier-width);
    min-width: var(--identifier-width);
    clear: left;
    float: left;
    text-align: right;
    margin-right: 1ch;
  }
  & dd {
    margin: 0;
    margin-left: calc(1em + var(--identifier-width)) !important;
    min-width: 5em;
  }
  & .authors {
    & .author {
      display: inline-block;
      margin-right: 1.5em;
    }
    & .org {
      font-style: italic;
    }
  }
}

/* The prepared/rendered info at the very bottom of the page */
.docInfo {
  color: #999;
  font-size: 0.9em;
  font-style: italic;
  margin-top: 2em;
}
.docInfo .prepared {
  float: right;
}

/* table of contents */
#toc {
  padding: 0.75em 0 2em 0;
  margin-bottom: 1em;

  & nav {
    & ul {
      margin: 0 0.5em 0 0;
      padding: 0;
      list-style: none;
    }
    & li {
      line-height: 1.3em;
      margin: 2px 0;
      padding-left: 1.2em;
      text-indent: -1.2em;
    }
  }
  & a.xref {
    white-space: normal;
  }
}

.references {
  & > dt {
    text-align: right;
    font-weight: bold;
    min-width: 10ch;
    margin-right: 1.5ch;
    &:target::before {
      content: "⇒";
      margin: 0 10px 0 -25px;
    }
  }
  & > dd {
    margin-left: 12ch !important;
    overflow: visible;
    & .refInstance {
      margin-bottom: 0.8em;
    }
    & .ascii {
      margin-bottom: 0.25em;
    }
  }
}

#rfc\.index\.index + ul {
  margin-left: 0;
}

/* authors */
address.vcard {
  font-style: normal;
  max-width: 20em;
  margin: 1em auto 1em 0;

  & .nameRole {
    font-weight: 700;
    margin-left: 0;
  }
  & .label {
    margin: 0.5em 0;
  }
  & .type {
    display: none;
  }
  & .alternative-contact {
    margin: 0.5em 0 0.25em 0;
  }
  & .non-ascii {
    margin: 0 0 0 2em;
  }
  & div.left {
    text-align: left;
  }
  & div.right {
    text-align: right;
  }
}

hr.addr {
  border-top: 1px dashed;
  margin: 0;
  color: #ddd;
  max-width: calc(100% - 16px);
}
@media (min-width: 500px) {
  #authors-addresses > section {
    column-count: 2;
    column-gap: 20px;
  }
  #authors-addresses > section > h2 {
    column-span: all;
  }
  /* hack for break-inside: avoid-column */
  #authors-addresses address {
    display: inline-block;
    break-inside: avoid-column;
  }
}

/* Comments */
.rfcEditorRemove p:first-of-type {
  font-style: italic;
}
.cref {
  background-color: rgba(249, 232, 105, 0.3);
  padding: 2px 4px;
}
.crefSource {
  font-style: italic;
}

@media screen {
  #toc nav {
    font-family: var(--font-title);
    font-weight: 360;
    & > ul { margin-bottom: 2em; }
    & ul {
      margin: 0 0 0 4px;
      & :is(p, li) {
        margin: 2px 0;
      }
    }
  }
  #toc a.toplink {
    float: right;
  }
}
@media not screen {
  #toc a.toplink {
    display: none;
  }
}


/* TOC layout for smaller screens */
@media screen and (max-width: 929px) {
  #toc {
    position: fixed;
    z-index: 2;
    top: 0;
    right: 0;
    padding: 1px 0 0 0;
    margin: 0;
    border-bottom: 1px solid #ccc;
    opacity: 0.6;
  }
  #toc h2 {
    margin: 0;
    padding: 2px 0 2px 6px;
    padding-right: 1em;
    font-size: 18px;
    line-height: 24px;
    min-width: 190px;
    text-align: right;
    background-color: #444;
    color: white;
    cursor: pointer;
    &::before { /* css hamburger */
      float: right;
      position: relative;
      width: 1em;
      height: 1px;
      left: -164px;
      margin: 8px 0 0 0;
      background: white none repeat scroll 0 0;
      box-shadow: 0 4px 0 0 white, 0 8px 0 0 white;
      content: "";
    }
  }
  #toc nav {
    display: none;
    background-color: var(--background-color);
    padding: 0.5em 1em 1em;
    overflow: auto;
    overscroll-behavior: contain;
    height: calc(100vh - 48px);
    border-left: 1px solid #ddd;
  }
  #toc.active {
    opacity: 1;
    & nav { display: block; }
  }
  /* Make the collapsed ToC header render white on gray also when it's a link */
  #toc h2 a,
  #toc h2 a:is(:link, :focus, :hover),
  #toc a.toplink,
  #toc a.toplink:hover {
    color: white;
    background-color: #444;
    text-decoration: none;
  }
  #toc a.toplink {
    margin: 2px 0.5em 0;
  }
}

/* TOC layout for wide screens */
@media screen and (min-width: 930px) {
  body {
    padding-right: 360px;
    padding-right: calc(min(180px + 20%, 500px));
  }
  #toc {
    position: fixed;
    bottom: 0;
    right: 0;
    right: calc(50vw - 480px);
    width: 312px;
    margin: 0;
    padding: 0;
    z-index: 1;
  }
  #toc h2 {
    margin: 0;
    padding: 0.25em 1em 1em 0;
  }
  #toc nav {
    display: block;
    height: calc(90vh - 84px);
    bottom: 0;
    padding: 0.5em 0 2em;
    overflow: auto;
    overscroll-behavior: contain;
    scrollbar-width: thin;
  }
  img { /* future proofing */
    max-width: 100%;
    height: auto;
  }
  #toc a.toplink {
    margin: 8px 0.5em 0;
  }
}

/* pagination */
@media print {
  body {
    width: 100%;
  }
  p {
    orphans: 3;
    widows: 3;
  }
  #n-copyright-notice {
    border-bottom: none;
  }
  #toc, #n-introduction {
    page-break-before: always;
  }
  #toc {
    border-top: none;
    padding-top: 0;
  }
  figure, pre, .vcard {
    page-break-inside: avoid;
  }
  h1, h2, h3, h4, h5, h6 {
    page-break-after: avoid;
  }
  :is(h2, h3, h4, h5, h6)+*, dd {
    page-break-before: avoid;
  }
  pre {
    white-space: pre-wrap;
    word-wrap: break-word;
    font-size: 10pt;
  }
  table {
    border: 1px solid #ddd;
  }
  td {
    border-top: 1px solid #ddd;
  }
}

@page :first {
  padding-top: 0;
  @top-left {
    content: normal;
    border: none;
  }
  @top-center {
    content: normal;
    border: none;
  }
  @top-right {
    content: normal;
    border: none;
  }
}

@page {
  size: A4;
  margin-bottom: 45mm;
  padding-top: 20px;
}

/* Dark mode. */
@media (prefers-color-scheme: dark) {
:root {
  --background-color: #121212;
  --text-color: #f0f0f0;
  --title-color: #fff;
  --link-color: #4da4f0;
  --highlight-color: #282828;
  --line-color: #444;
  --pilcrow-weak: #444;
  --pilcrow-strong: #666;
  scrollbar-color: #777 #333;
}
}

/* SVG Trick: a prefix match works because only black and white are allowed */
svg :is([stroke="black"], [stroke^="#000"]) {
  stroke: var(--text-color);
}
svg :is([stroke="white"], [stroke^="#fff"]) {
  stroke: var(--background-color);
}
svg :is([fill="black"], [fill^="#000"], :not([fill])) {
  fill: var(--text-color);
}
svg :is([fill="white"], [fill^="#fff"]) {
  fill: var(--background-color);
}
</style>

</head>
<body class="xml2rfc">
<table class="ears">
<thead><tr>
<td class="left">Internet-Draft</td>
<td class="center">Source Buffer Management</td>
<td class="right">March 2025</td>
</tr></thead>
<tfoot><tr>
<td class="left">Cheshire</td>
<td class="center">Expires 4 September 2025</td>
<td class="right">[Page]</td>
</tr></tfoot>
</table>
<div id="external-metadata" class="document-information"></div>
<div id="internal-metadata" class="document-information">
<dl id="identifiers">
<dt class="label-workgroup">Workgroup:</dt>
<dd class="workgroup">Network Working Group</dd>
<dt class="label-internet-draft">Internet-Draft:</dt>
<dd class="internet-draft">draft-cheshire-sbm-latest</dd>
<dt class="label-published">Published:</dt>
<dd class="published">
<time datetime="2025-03-03" class="published">3 March 2025</time>
    </dd>
<dt class="label-intended-status">Intended Status:</dt>
<dd class="intended-status">Informational</dd>
<dt class="label-expires">Expires:</dt>
<dd class="expires"><time datetime="2025-09-04">4 September 2025</time></dd>
<dt class="label-authors">Author:</dt>
<dd class="authors">
<div class="author">
      <div class="author-name">S. Cheshire</div>
<div class="org">Apple Inc.</div>
</div>
</dd>
</dl>
</div>
<h1 id="title">Source Buffer Management</h1>
<section id="section-abstract">
      <h2 id="abstract"><a href="#abstract" class="selfRef">Abstract</a></h2>
<p id="section-abstract-1">In the past decade there has been growing awareness about the
harmful effects of bufferbloat in the network, and there has
been good work on developments like L4S to address that problem.
However, bufferbloat on the sender itself remains a significant
additional problem, which has not received similar attention.
This document offers techniques and guidance for host networking
software to avoid network traffic suffering unnecessary delays
caused by excessive buffering at the sender. These improvements
are broadly applicable across all datagram and transport
protocols (UDP, TCP, QUIC, etc.) on all operating systems.<a href="#section-abstract-1" class="pilcrow">¶</a></p>
</section>
<section class="note rfcEditorRemove" id="section-note.1">
      <h2 id="name-about-this-document">
<a href="#name-about-this-document" class="section-name selfRef">About This Document</a>
      </h2>
<p id="section-note.1-1">This note is to be removed before publishing as an RFC.<a href="#section-note.1-1" class="pilcrow">¶</a></p>
<p id="section-note.1-2">
        The latest revision of this draft can be found at <span><a href="https://StuartCheshire.github.io/draft-cheshire-sbm/draft-cheshire-sbm.html">https://StuartCheshire.github.io/draft-cheshire-sbm/draft-cheshire-sbm.html</a></span>.
        Status information for this document may be found at <span><a href="https://datatracker.ietf.org/doc/draft-cheshire-sbm/">https://datatracker.ietf.org/doc/draft-cheshire-sbm/</a></span>.<a href="#section-note.1-2" class="pilcrow">¶</a></p>
<p id="section-note.1-3">Source for this draft and an issue tracker can be found at
        <span><a href="https://github.com/StuartCheshire/draft-cheshire-sbm">https://github.com/StuartCheshire/draft-cheshire-sbm</a></span>.<a href="#section-note.1-3" class="pilcrow">¶</a></p>
</section>
<div id="status-of-memo">
<section id="section-boilerplate.1">
        <h2 id="name-status-of-this-memo">
<a href="#name-status-of-this-memo" class="section-name selfRef">Status of This Memo</a>
        </h2>
<p id="section-boilerplate.1-1">
        This Internet-Draft is submitted in full conformance with the
        provisions of BCP 78 and BCP 79.<a href="#section-boilerplate.1-1" class="pilcrow">¶</a></p>
<p id="section-boilerplate.1-2">
        Internet-Drafts are working documents of the Internet Engineering Task
        Force (IETF). Note that other groups may also distribute working
        documents as Internet-Drafts. The list of current Internet-Drafts is
        at <span><a href="https://datatracker.ietf.org/drafts/current/">https://datatracker.ietf.org/drafts/current/</a></span>.<a href="#section-boilerplate.1-2" class="pilcrow">¶</a></p>
<p id="section-boilerplate.1-3">
        Internet-Drafts are draft documents valid for a maximum of six months
        and may be updated, replaced, or obsoleted by other documents at any
        time. It is inappropriate to use Internet-Drafts as reference
        material or to cite them other than as "work in progress."<a href="#section-boilerplate.1-3" class="pilcrow">¶</a></p>
<p id="section-boilerplate.1-4">
        This Internet-Draft will expire on 4 September 2025.<a href="#section-boilerplate.1-4" class="pilcrow">¶</a></p>
</section>
</div>
<div id="copyright">
<section id="section-boilerplate.2">
        <h2 id="name-copyright-notice">
<a href="#name-copyright-notice" class="section-name selfRef">Copyright Notice</a>
        </h2>
<p id="section-boilerplate.2-1">
            Copyright (c) 2025 IETF Trust and the persons identified as the
            document authors. All rights reserved.<a href="#section-boilerplate.2-1" class="pilcrow">¶</a></p>
<p id="section-boilerplate.2-2">
            This document is subject to BCP 78 and the IETF Trust's Legal
            Provisions Relating to IETF Documents
            (<span><a href="https://trustee.ietf.org/license-info">https://trustee.ietf.org/license-info</a></span>) in effect on the date of
            publication of this document. Please review these documents
            carefully, as they describe your rights and restrictions with
            respect to this document.<a href="#section-boilerplate.2-2" class="pilcrow">¶</a></p>
</section>
</div>
<div id="toc">
<section id="section-toc.1">
        <a href="#" onclick="scroll(0,0)" class="toplink">▲</a><h2 id="name-table-of-contents">
<a href="#name-table-of-contents" class="section-name selfRef">Table of Contents</a>
        </h2>
<nav class="toc"><ul class="compact toc ulBare ulEmpty">
<li class="compact toc ulBare ulEmpty" id="section-toc.1-1.1">
            <p id="section-toc.1-1.1.1" class="keepWithNext"><a href="#section-1" class="auto internal xref">1</a>.  <a href="#name-conventions-and-definitions" class="internal xref">Conventions and Definitions</a></p>
</li>
          <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.2">
            <p id="section-toc.1-1.2.1" class="keepWithNext"><a href="#section-2" class="auto internal xref">2</a>.  <a href="#name-introduction" class="internal xref">Introduction</a></p>
</li>
          <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.3">
            <p id="section-toc.1-1.3.1"><a href="#section-3" class="auto internal xref">3</a>.  <a href="#name-source-buffer-backpressure" class="internal xref">Source Buffer Backpressure</a></p>
<ul class="compact toc ulBare ulEmpty">
<li class="compact toc ulBare ulEmpty" id="section-toc.1-1.3.2.1">
                <p id="section-toc.1-1.3.2.1.1" class="keepWithNext"><a href="#section-3.1" class="auto internal xref">3.1</a>.  <a href="#name-direct-backpressure" class="internal xref">Direct Backpressure</a></p>
</li>
              <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.3.2.2">
                <p id="section-toc.1-1.3.2.2.1"><a href="#section-3.2" class="auto internal xref">3.2</a>.  <a href="#name-indirect-backpressure" class="internal xref">Indirect Backpressure</a></p>
</li>
            </ul>
</li>
          <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.4">
            <p id="section-toc.1-1.4.1"><a href="#section-4" class="auto internal xref">4</a>.  <a href="#name-case-study-tcp_notsent_lowa" class="internal xref">Case Study -- TCP_NOTSENT_LOWAT</a></p>
</li>
          <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.5">
            <p id="section-toc.1-1.5.1"><a href="#section-5" class="auto internal xref">5</a>.  <a href="#name-shortcomings-of-tcp_notsent" class="internal xref">Shortcomings of TCP_NOTSENT_LOWAT</a></p>
<ul class="compact toc ulBare ulEmpty">
<li class="compact toc ulBare ulEmpty" id="section-toc.1-1.5.2.1">
                <p id="section-toc.1-1.5.2.1.1"><a href="#section-5.1" class="auto internal xref">5.1</a>.  <a href="#name-platform-differences" class="internal xref">Platform Differences</a></p>
</li>
              <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.5.2.2">
                <p id="section-toc.1-1.5.2.2.1"><a href="#section-5.2" class="auto internal xref">5.2</a>.  <a href="#name-time-versus-bytes" class="internal xref">Time versus Bytes</a></p>
</li>
              <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.5.2.3">
                <p id="section-toc.1-1.5.2.3.1"><a href="#section-5.3" class="auto internal xref">5.3</a>.  <a href="#name-other-transport-protocols" class="internal xref">Other Transport Protocols</a></p>
</li>
            </ul>
</li>
          <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.6">
            <p id="section-toc.1-1.6.1"><a href="#section-6" class="auto internal xref">6</a>.  <a href="#name-tcp_replenish_time" class="internal xref">TCP_REPLENISH_TIME</a></p>
<ul class="compact toc ulBare ulEmpty">
<li class="compact toc ulBare ulEmpty" id="section-toc.1-1.6.2.1">
                <p id="section-toc.1-1.6.2.1.1"><a href="#section-6.1" class="auto internal xref">6.1</a>.  <a href="#name-solicitation-for-name-sugge" class="internal xref">Solicitation for Name Suggestions</a></p>
</li>
            </ul>
</li>
          <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.7">
            <p id="section-toc.1-1.7.1"><a href="#section-7" class="auto internal xref">7</a>.  <a href="#name-applicability" class="internal xref">Applicability</a></p>
<ul class="compact toc ulBare ulEmpty">
<li class="compact toc ulBare ulEmpty" id="section-toc.1-1.7.2.1">
                <p id="section-toc.1-1.7.2.1.1"><a href="#section-7.1" class="auto internal xref">7.1</a>.  <a href="#name-physical-bottlenecks" class="internal xref">Physical Bottlenecks</a></p>
</li>
              <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.7.2.2">
                <p id="section-toc.1-1.7.2.2.1"><a href="#section-7.2" class="auto internal xref">7.2</a>.  <a href="#name-algorithmic-bottlenecks" class="internal xref">Algorithmic Bottlenecks</a></p>
</li>
              <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.7.2.3">
                <p id="section-toc.1-1.7.2.3.1"><a href="#section-7.3" class="auto internal xref">7.3</a>.  <a href="#name-superiority-of-direct-backp" class="internal xref">Superiority of Direct Backpressure</a></p>
</li>
              <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.7.2.4">
                <p id="section-toc.1-1.7.2.4.1"><a href="#section-7.4" class="auto internal xref">7.4</a>.  <a href="#name-application-programming-int" class="internal xref">Application Programming Interface</a></p>
</li>
              <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.7.2.5">
                <p id="section-toc.1-1.7.2.5.1"><a href="#section-7.5" class="auto internal xref">7.5</a>.  <a href="#name-relationship-between-throug" class="internal xref">Relationship Between Throughput and Delay</a></p>
</li>
              <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.7.2.6">
                <p id="section-toc.1-1.7.2.6.1"><a href="#section-7.6" class="auto internal xref">7.6</a>.  <a href="#name-bulk-transfer-protocols" class="internal xref">Bulk Transfer Protocols</a></p>
</li>
            </ul>
</li>
          <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.8">
            <p id="section-toc.1-1.8.1"><a href="#section-8" class="auto internal xref">8</a>.  <a href="#name-alternative-proposals" class="internal xref">Alternative Proposals</a></p>
<ul class="compact toc ulBare ulEmpty">
<li class="compact toc ulBare ulEmpty" id="section-toc.1-1.8.2.1">
                <p id="section-toc.1-1.8.2.1.1"><a href="#section-8.1" class="auto internal xref">8.1</a>.  <a href="#name-just-use-udp" class="internal xref">Just use UDP</a></p>
</li>
              <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.8.2.2">
                <p id="section-toc.1-1.8.2.2.1"><a href="#section-8.2" class="auto internal xref">8.2</a>.  <a href="#name-packet-expiration" class="internal xref">Packet Expiration</a></p>
</li>
              <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.8.2.3">
                <p id="section-toc.1-1.8.2.3.1"><a href="#section-8.3" class="auto internal xref">8.3</a>.  <a href="#name-head-of-line-blocking-traff" class="internal xref">Head of Line Blocking / Traffic Priorities</a></p>
</li>
            </ul>
</li>
          <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.9">
            <p id="section-toc.1-1.9.1"><a href="#section-9" class="auto internal xref">9</a>.  <a href="#name-security-considerations" class="internal xref">Security Considerations</a></p>
</li>
          <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.10">
            <p id="section-toc.1-1.10.1"><a href="#section-10" class="auto internal xref">10</a>. <a href="#name-iana-considerations" class="internal xref">IANA Considerations</a></p>
</li>
          <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.11">
            <p id="section-toc.1-1.11.1"><a href="#section-11" class="auto internal xref">11</a>. <a href="#name-references" class="internal xref">References</a></p>
<ul class="compact toc ulBare ulEmpty">
<li class="compact toc ulBare ulEmpty" id="section-toc.1-1.11.2.1">
                <p id="section-toc.1-1.11.2.1.1"><a href="#section-11.1" class="auto internal xref">11.1</a>.  <a href="#name-normative-references" class="internal xref">Normative References</a></p>
</li>
              <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.11.2.2">
                <p id="section-toc.1-1.11.2.2.1"><a href="#section-11.2" class="auto internal xref">11.2</a>.  <a href="#name-informative-references" class="internal xref">Informative References</a></p>
</li>
            </ul>
</li>
          <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.12">
            <p id="section-toc.1-1.12.1"><a href="#appendix-A" class="auto internal xref"></a><a href="#name-acknowledgments" class="internal xref">Acknowledgments</a></p>
</li>
          <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.13">
            <p id="section-toc.1-1.13.1"><a href="#appendix-B" class="auto internal xref"></a><a href="#name-authors-address" class="internal xref">Author's Address</a></p>
</li>
        </ul>
</nav>
</section>
</div>
<div id="conventions-and-definitions">
<section id="section-1">
      <h2 id="name-conventions-and-definitions">
<a href="#section-1" class="section-number selfRef">1. </a><a href="#name-conventions-and-definitions" class="section-name selfRef">Conventions and Definitions</a>
      </h2>
<p id="section-1-1">The key words "<span class="bcp14">MUST</span>", "<span class="bcp14">MUST NOT</span>", "<span class="bcp14">REQUIRED</span>", "<span class="bcp14">SHALL</span>", "<span class="bcp14">SHALL NOT</span>", "<span class="bcp14">SHOULD</span>", "<span class="bcp14">SHOULD NOT</span>", "<span class="bcp14">RECOMMENDED</span>", "<span class="bcp14">NOT RECOMMENDED</span>",
"<span class="bcp14">MAY</span>", and "<span class="bcp14">OPTIONAL</span>" in this document are to be interpreted as
described in BCP 14 <span>[<a href="#RFC2119" class="cite xref">RFC2119</a>]</span> <span>[<a href="#RFC8174" class="cite xref">RFC8174</a>]</span> when, and only when, they
appear in all capitals, as shown here.<a href="#section-1-1" class="pilcrow">¶</a></p>
</section>
</div>
<div id="introduction">
<section id="section-2">
      <h2 id="name-introduction">
<a href="#section-2" class="section-number selfRef">2. </a><a href="#name-introduction" class="section-name selfRef">Introduction</a>
      </h2>
<p id="section-2-1">In 2010 Jim Gettys identified the problem
of how excessive buffering in networks adversely affects
delay-sensitive applications <span>[<a href="#Bloat1" class="cite xref">Bloat1</a>]</span><span>[<a href="#Bloat2" class="cite xref">Bloat2</a>]</span><span>[<a href="#Bloat3" class="cite xref">Bloat3</a>]</span>.
This important work identifying a non-obvious problem
has led to valuable developments to improve this situation,
like fq_codel <span>[<a href="#RFC8290" class="cite xref">RFC8290</a>]</span>, PIE <span>[<a href="#RFC8033" class="cite xref">RFC8033</a>]</span>, Cake <span>[<a href="#Cake" class="cite xref">Cake</a>]</span>
and L4S <span>[<a href="#RFC9330" class="cite xref">RFC9330</a>]</span>.<a href="#section-2-1" class="pilcrow">¶</a></p>
<p id="section-2-2">However, excessive buffering at the source
-- in the sending devices themselves --
can equally contribute to degraded performance
for delay-sensitive applications,
and this problem has not yet received
a similar level of attention.<a href="#section-2-2" class="pilcrow">¶</a></p>
<p id="section-2-3">This document describes the source buffering problem,
steps that have been taken so far to address the problem,
shortcomings with those existing solutions,
and new mechanisms that work better.<a href="#section-2-3" class="pilcrow">¶</a></p>
<p id="section-2-4">To explain the problem and the solution,
this document begins with some historical background
about why computers have buffers in the first place,
and why buffers are useful.
This document explains the need for backpressure on
senders that are able to exceed the network capacity,
and separates backpressure mechanisms into
direct backpressure and indirect backpressure.<a href="#section-2-4" class="pilcrow">¶</a></p>
<p id="section-2-5">The document describes
the TCP_REPLENISH_TIME socket option
for TCP connections using BSD Sockets,
and its equivalent for other networking protocols and APIs.<a href="#section-2-5" class="pilcrow">¶</a></p>
<p id="section-2-6">The goal is for application software to be able to
write chunks of data large enough to be efficient,
without writing too many of them too quickly.
This avoids the unfortunate situation where a delay-sensitive
application inadvertently writes many blocks of data
long before they will actually depart the source machine,
such that by the time the enqueued data is actually sent,
the application may have newer data that it would rather send instead.
By deferring generating data until the networking code is
actually ready to send it, the application retains more precise
control over what data will be sent when the opportunity arises.<a href="#section-2-6" class="pilcrow">¶</a></p>
<p id="section-2-7">The document concludes by describing some alternative
solutions that are often proposed, and explains
why we feel they are less effective than simply
implementing effective source buffer management.<a href="#section-2-7" class="pilcrow">¶</a></p>
</section>
</div>
<div id="source-buffer-backpressure">
<section id="section-3">
      <h2 id="name-source-buffer-backpressure">
<a href="#section-3" class="section-number selfRef">3. </a><a href="#name-source-buffer-backpressure" class="section-name selfRef">Source Buffer Backpressure</a>
      </h2>
<p id="section-3-1">Starting with the most basic principles,
computers have always had to deal with the situation
where software is able to generate output data
faster than the physical medium can accept it.
The software may be sending data to a paper tape punch,
to an RS232 serial port (UART),
or to a printer connected via a parallel port.
The software may be writing data to a floppy disk
or a spinning hard disk.
It was self-evident to early computer designers that it would
be unacceptable for data to be lost in these cases.<a href="#section-3-1" class="pilcrow">¶</a></p>
<div id="direct-backpressure">
<section id="section-3.1">
        <h3 id="name-direct-backpressure">
<a href="#section-3.1" class="section-number selfRef">3.1. </a><a href="#name-direct-backpressure" class="section-name selfRef">Direct Backpressure</a>
        </h3>
<p id="section-3.1-1">The early solutions were simple.
When an application wrote data to a file on a floppy disk,
the file system “write” API would not return control to the caller
until the data had actually been written to the floppy disk.
This had the natural effect of slowing down
the application so that it could not exceed
the capacity of the medium to accept the data.<a href="#section-3.1-1" class="pilcrow">¶</a></p>
<p id="section-3.1-2">Soon it became clear that these simple synchronous APIs
unreasonably limited the performance of the system.
If, instead, the file system “write” API
were to return to the caller immediately
-- even though the actual write to the
spinning hard disk had not yet completed --
then the application could get on with other
useful work while the actual write to the
spinning hard disk proceeded in parallel.<a href="#section-3.1-2" class="pilcrow">¶</a></p>
<p id="section-3.1-3">Some systems allowed a single asynchronous write
to the spinning hard disk to proceed while
the application software performed other processing.
Other systems allowed multiple asynchronous writes to be enqueued,
but even these systems generally imposed some upper bound on
the amount of outstanding incomplete writes they would support.
At some point, if the application software persisted in
trying to write data faster than the medium could accept it,
then the application would be throttled in some way,
either by making the API call a blocking call
(simply not returning control to the application,
removing its ability to do anything else)
or by returning a Unix EWOULDBLOCK error or similar
(to inform the application that its API call had
been unsuccessful, and that it would need to take
action to write its data again at a later time).<a href="#section-3.1-3" class="pilcrow">¶</a></p>
<p id="section-3.1-4">It is informative to observe a comparison with graphics cards.
Most graphics cards support double-buffering.
This allows one frame to be displayed while
the CPU and GPU are working on generating the next frame.
This concurrency allows for greater efficiency,
by enabling two actions to be happening at the same time.
But quintuple-buffering is not better than double-buffering.
Having a pipeline five frames deep, or ten frames,
or fifty frames, is not better than two frames.
For a fast-paced video game, having a display pipeline fifty
frames deep, where every frame is generated, then waits in
the pipeline, and then is displayed fifty frames later,
would not improve performance or efficiency,
but would cause an unacceptable delay between
a player performing an action and
seeing the results of that action on the screen.
It is beneficial for the video game to work on preparing
the next frame while the previous frame is being displayed,
but it is not beneficial for the video game to get multiple
frames ahead of the frame currently being displayed.<a href="#section-3.1-4" class="pilcrow">¶</a></p>
<p id="section-3.1-5">Another reason that it is good not to permit an
excessive amount of unsent data to be queued up
is that once data is committed to a buffer,
there are generally limited options for changing it.
Some systems may provide a mechanism to flush the entire
buffer and discard all the data, but mechanisms to
selectively remove or re-order enqueued data
are complicated and rare.
While it could be possible to add such mechanisms,
on balance it is simpler simply to avoid committing
too much unsent data to the buffer in the first place.
If the backlog of unsent data is kept reasonably low,
that gives the source more flexibility decide what to
put into the buffer next, when that opportunity arises.<a href="#section-3.1-5" class="pilcrow">¶</a></p>
<p id="section-3.1-6">In summary, in order to give applications maximum
flexibility, pending data should be kept as close
to the application for as long as possible.
Application buffers should be as large as needed
for the application to do its work,
and lower-layer buffers should be no larger than
is necessary to provide efficient use of available
network capacity and other resources like CPU time.<a href="#section-3.1-6" class="pilcrow">¶</a></p>
</section>
</div>
<div id="indirect-backpressure">
<section id="section-3.2">
        <h3 id="name-indirect-backpressure">
<a href="#section-3.2" class="section-number selfRef">3.2. </a><a href="#name-indirect-backpressure" class="section-name selfRef">Indirect Backpressure</a>
        </h3>
<p id="section-3.2-1">All of the situations described above using “direct backpressure”
are one-hop communication where the CPU generating the data
is connected more-or-less directly to the device consuming the data.
In these cases it is relatively simple for the receiving device
to exert direct backpressure to influence
the rate at which the CPU sends data.<a href="#section-3.2-1" class="pilcrow">¶</a></p>
<p id="section-3.2-2">When we introduce multi-hop networking,
the situation becomes more complicated.
When a flow of packets travels 30 hops though
a network, the bottleneck hop may be quite distant
from the original source of the data stream.<a href="#section-3.2-2" class="pilcrow">¶</a></p>
<p id="section-3.2-3">For example, consider the case of
a smartphone communicating via a Wi-Fi Access Point at 600 Mb/s,
which is connected to a home NAT gateway via gigabit Ethernet,
which is connected to a cable modem via gigabit Ethernet,
which has an upstream output rate of 35Mb/s over the coaxial cable.
When the cable modem experiences
an excessive flow of incoming packets arriving
on its gigabit Ethernet interface,
the cable modem has no direct way to cause
the networking code on the smartphone to curtail
the influx of data by pausing the sending application
via blocking its write calls or delivering EWOULDBLOCK errors.
The source of the excessive flood of data
causing the problem (the smartphone)
is three network hops away from the device
experiencing the problem (the cable modem).
When an incoming packet arrives,
the cable modem’s choices are limited to
enqueueing the packet,
discarding the packet,
or enqueueing the packet and
marking it with an ECN CE mark <span>[<a href="#RFC3168" class="cite xref">RFC3168</a>]</span> <span>[<a href="#RFC9330" class="cite xref">RFC9330</a>]</span>.
The cable modem drops or marks an incoming packet
in the expectation that this will, eventually,
indirectly, cause the networking code and operating system
on the sending device to take the necessary steps
to curtail the sending application.<a href="#section-3.2-3" class="pilcrow">¶</a></p>
<p id="section-3.2-4">The reasons the cable modem’s choices are so limited
are because of security and packet size constraints.<a href="#section-3.2-4" class="pilcrow">¶</a></p>
<p id="section-3.2-5">Security and trust concerns revolve around preventing a
malicious entity from performing a denial-of-service attack
against a victim device by sending fraudulent messages that
would cause it to reduce its transmission rate.
It is particularly important to guard against an off-path attacker
being able to do this. This concern is addressed if queue size
feedback generated in the network follows the same path already
taken by the data packets and their subsequent acknowledgement
packets. The logic is that any on-path device that is able to
modify data packets (changing the ECN bits in the IP header)
could equally well corrupt packets or discard them entirely.
Thus, trusting ECN information from these devices does not
increase security concerns, since these devices could already
perform more malicious actions anyway. The sender already
trusts the receiver to generate accurate acknowledgement
packets, so also trusting it to report ECN information back
to the sender does not increase the security risk.<a href="#section-3.2-5" class="pilcrow">¶</a></p>
<p id="section-3.2-6">A consequence of this security requirement is that it takes a
full round trip time for the source to learn about queue state
in the network. In many common cases this is not a significant
deficiency. For example, if a user is receiving data from a
well-connected server on the Internet, and the network
bottleneck is the last hop on the path (e.g., the Wi-Fi hop to
the user’s smartphone in their home) then the location where
the queue is building up (the Wi-Fi Access Point) is very close
to the receiver, and having the receiver echo the queue state
information back to the sender does not add significant delay.<a href="#section-3.2-6" class="pilcrow">¶</a></p>
<p id="section-3.2-7">Packet size constraints, particularly scarce bits available
in the IP header, mean that for pragmatic reasons the ECN
queue size feedback is limited to two states: “The source
may try sending a little faster if desired,” and, “The
source should reduce its sending rate.” Use of these
increase/decrease indications in successive packets allows
the sender to converge on the ideal transmission rate, and
then to oscillate slightly around the ideal transmission
rate as it continues to track changing network conditions.<a href="#section-3.2-7" class="pilcrow">¶</a></p>
<p id="section-3.2-8">Discarding or marking an incoming packet
at some point within the network are
what we refer to as indirect backpressure,
with the assumption that these actions will eventually
result in the sending application being throttled
via having a write call blocked,
returning an EWOULDBLOCK error,
or some other form of backpressure that
causes the source application
to temporarily pause sending new data.<a href="#section-3.2-8" class="pilcrow">¶</a></p>
</section>
</div>
</section>
</div>
<div id="casestudy">
<section id="section-4">
      <h2 id="name-case-study-tcp_notsent_lowa">
<a href="#section-4" class="section-number selfRef">4. </a><a href="#name-case-study-tcp_notsent_lowa" class="section-name selfRef">Case Study -- TCP_NOTSENT_LOWAT</a>
      </h2>
<p id="section-4-1">In April 2011 the author was investigating
sluggishness with Mac OS Screen Sharing,
which uses the VNC Remote Framebuffer (RFB) protocol <span>[<a href="#RFC6143" class="cite xref">RFC6143</a>]</span>.
Initially it seemed like a classic case of network bufferbloat.
However, deeper investigation revealed that in this case
the network was not responsible for the excessive delay --
the excessive delay was being caused by
excessive buffering on the sending device itself.<a href="#section-4-1" class="pilcrow">¶</a></p>
<p id="section-4-2">In this case the network connection was a relatively slow
DSL line (running at about 500 kb/s) and
the socket send buffer (SO_SNDBUF) was set to 128 kilobytes.
With a 50 ms round-trip time,
about 3 kilobytes (roughly two packets)
was sufficient to fill the bandwidth-delay product of the path.
The remaining 125 kilobytes available in the 128 kB socket send buffer
were simply holding bytes that had not even been sent yet.
At 500 kb/s throughput (62.5 kB/s),
this meant that every byte written by the VNC RFB server
spent two seconds sitting in the socket send buffer
before it even left the source machine.
Clearly, delaying every sent byte by two seconds
resulted in a very sluggish screen sharing experience,
and it did not yield any useful benefit like
higher throughput or lower CPU utilization.<a href="#section-4-2" class="pilcrow">¶</a></p>
<p id="section-4-3">This led to the creation in May 2011
of a new socket option on Mac OS and iOS
called “TCP_NOTSENT_LOWAT”.
This new socket option provided the ability for
sending software (like the VNC RFB server)
to specify a low-water-mark threshold for the
minimum amount of <strong>unsent</strong> data it would like
to have waiting in the socket send buffer.
Instead of encouraging the application to
fill the socket send buffer to its maximum capacity,
the socket send buffer would hold just the data
that had been sent but not yet acknowledged
(enough to fully occupy the bandwidth-delay product
of the network path and fully utilize the available capacity)
plus some <strong>small</strong> amount of additional unsent data waiting to go out.
Some <strong>small</strong> amount of unsent data waiting to go out is
beneficial, so that the network stack has data
ready to send when the opportunity arises
(e.g., a TCP ACK arrives signalling
that previous data has now been delivered).
Too much unsent data waiting to go out
-- in excess of what the network stack
might soon be able to send --
is harmful for delay-sensitive applications
because it increases delay without
meaningfully increasing throughput or utilization.<a href="#section-4-3" class="pilcrow">¶</a></p>
<p id="section-4-4">Empirically it was found that setting an
unsent data low-water-mark threshold of 16 kilobytes
worked well for VNC RFB screen sharing.
When the amount of unsent data fell below this
low-water-mark threshold, kevent() would
wake up the VNC RFB screen sharing application
to begin work on preparing the next frame to send.
Once the VNC RFB screen sharing application
had prepared the next frame and written it
to the socket send buffer,
it would again call kevent() to block and wait
to be notified when it became time to begin work
on the following frame.
This allows the VNC RFB screen sharing server
to stay just one frame ahead of
the frame currently being sent over the network,
and not inadvertently get multiple frames ahead.
This provided enough unsent data waiting to go out
to fully utilize the capacity of the path,
without buffering so much unsent data
that it adversely affected usability.<a href="#section-4-4" class="pilcrow">¶</a></p>
<p id="section-4-5">A live on-stage demo showing the benefits of using TCP_NOTSENT_LOWAT
with VNC RFB screen sharing was shown at the
Apple Worldwide Developer Conference in June 2015 <span>[<a href="#Demo" class="cite xref">Demo</a>]</span>.<a href="#section-4-5" class="pilcrow">¶</a></p>
</section>
</div>
<div id="shortcomings-of-tcpnotsentlowat">
<section id="section-5">
      <h2 id="name-shortcomings-of-tcp_notsent">
<a href="#section-5" class="section-number selfRef">5. </a><a href="#name-shortcomings-of-tcp_notsent" class="section-name selfRef">Shortcomings of TCP_NOTSENT_LOWAT</a>
      </h2>
<p id="section-5-1">While TCP_NOTSENT_LOWAT achieved its initial intended goal,
later operational experience has revealed some shortcomings.<a href="#section-5-1" class="pilcrow">¶</a></p>
<div id="platform-differences">
<section id="section-5.1">
        <h3 id="name-platform-differences">
<a href="#section-5.1" class="section-number selfRef">5.1. </a><a href="#name-platform-differences" class="section-name selfRef">Platform Differences</a>
        </h3>
<p id="section-5.1-1">The Linux network maintainers implemented a TCP
socket option with the same name, but different behavior.
While the Apple version of TCP_NOTSENT_LOWAT was
focussed on reducing delay,
the Linux version was focussed on reducing kernel memory usage.
The Apple version of TCP_NOTSENT_LOWAT controls
a low-water mark, below which the application is signalled
that it is time to begin working on generating fresh data.
The Linux version determines a high-water mark for unsent data,
above which the application is <strong>prevented</strong> from writing any more,
even if it has data prepared and ready to enqueue.
Setting TCP_NOTSENT_LOWAT to 16 kilobytes works well on Apple
systems, but can severely limit throughput on Linux systems.
This has led to confusion among developers and makes it difficult
to write portable code that works on both platforms.<a href="#section-5.1-1" class="pilcrow">¶</a></p>
</section>
</div>
<div id="time-versus-bytes">
<section id="section-5.2">
        <h3 id="name-time-versus-bytes">
<a href="#section-5.2" class="section-number selfRef">5.2. </a><a href="#name-time-versus-bytes" class="section-name selfRef">Time versus Bytes</a>
        </h3>
<p id="section-5.2-1">The original thinking on TCP_NOTSENT_LOWAT focussed on
the number of unsent bytes remaining, but it soon became
clear that the relevant quantity was time, not bytes.
The quantity of interest to the sending application
was how much advance notice it would get of impending
data exhaustion, so that it would have enough time
to generate its next logical block of data.
On low-rate paths (e.g., 250 kb/s and less)
16 kilobytes of unsent data could still result
in a fairly significant unnecessary queueing delay.
On high-rate paths (e.g., Gb/s and above)
16 kilobytes of unsent data could be consumed
very quickly, leaving the sending application
insufficient time to generate its next logical block of data
before the unsent backlog ran out
and available network capacity was left unused.
It became clear that it would be more useful for the
sending application specify how much advance notice
of data exhaustion it required (in milliseconds, or microseconds),
depending on how much time the application anticipated
needing to generate its next logical block of data.<a href="#section-5.2-1" class="pilcrow">¶</a></p>
<p id="section-5.2-2">The application could perform this calculation itself,
calculating the estimated current data rate and dividing
that by its desired advance notice time, to compute the number
of outstanding unsent bytes corresponding to that desired time.
However, the application would have to keep adjusting its
TCP_NOTSENT_LOWAT value as the observed data rate changed.
Since the transport protocol already knows the number of
unacknowledged bytes in flight, and the current round-trip delay,
the transport protocol is in a better position
to perform this calculation.<a href="#section-5.2-2" class="pilcrow">¶</a></p>
<p id="section-5.2-3">In addition, the network stack knows if features like hardware
offload, aggregation, and stretch acks are being used,
which could impact the burstiness of consumption of unsent bytes.<a href="#section-5.2-3" class="pilcrow">¶</a></p>
<p id="section-5.2-4">Wi-Fi interfaces perform better when they send
batches of packets aggregated together instead of
sending individual packets one at a time.
The amount of aggregation that is desirable depends
on the current wireless conditions,
so the Wi-Fi interface and its driver
are in the best position to determine that.<a href="#section-5.2-4" class="pilcrow">¶</a></p>
<p id="section-5.2-5">If stretch acks are being used, then each ack packet
could acknowledge 8 data segments, or about 12 kilobytes.
If one such ack packet is lost, the following ack packet
will cumulatively acknowledge 24 kilobytes,
instantly consuming the entire 16 kilobyte unsent backlog,
and giving the application no advance notice that
the transport protocol is suddenly out of available data to send,
and some network capacity becomes wasted.<a href="#section-5.2-5" class="pilcrow">¶</a></p>
<p id="section-5.2-6">Occasional failures to fully utilize the entire
available network capacity are not a disaster, but we
still would like to avoid this being a common occurrence.
Therefore it is better to have the transport protocol,
in cooperation with the other layers of the network stack,
use all the information available to estimate
when it expects to run out of data available to send,
given the current network conditions
and current amount of unsent data.
When the estimated time remaining until exhaustion falls
below the application’s specified threshold, the application
is notified to begin working on generating more data.<a href="#section-5.2-6" class="pilcrow">¶</a></p>
</section>
</div>
<div id="other-transport-protocols">
<section id="section-5.3">
        <h3 id="name-other-transport-protocols">
<a href="#section-5.3" class="section-number selfRef">5.3. </a><a href="#name-other-transport-protocols" class="section-name selfRef">Other Transport Protocols</a>
        </h3>
<p id="section-5.3-1">TCP_NOTSENT_LOWAT was initially defined only for TCP,
and only for the BSD Sockets programming interface.
It would be useful to define equivalent delay management
capabilities for other transport protocols,
like QUIC <span>[<a href="#RFC9000" class="cite xref">RFC9000</a>]</span><span>[<a href="#RFC9369" class="cite xref">RFC9369</a>]</span>,
and for other network programming APIs.<a href="#section-5.3-1" class="pilcrow">¶</a></p>
</section>
</div>
</section>
</div>
<div id="tcpreplenishtime">
<section id="section-6">
      <h2 id="name-tcp_replenish_time">
<a href="#section-6" class="section-number selfRef">6. </a><a href="#name-tcp_replenish_time" class="section-name selfRef">TCP_REPLENISH_TIME</a>
      </h2>
<p id="section-6-1">Because of these lessons learned, this document proposes
a new BSD Socket option for TCP, TCP_REPLENISH_TIME.<a href="#section-6-1" class="pilcrow">¶</a></p>
<p id="section-6-2">The new TCP_REPLENISH_TIME socket option specifies the
threshold for notifying an application of impending data
exhaustion in terms of microseconds, not bytes.
It is the job of the transport protocol to compute its
best estimate of when the amount of remaining unsent data
falls below this threshold.<a href="#section-6-2" class="pilcrow">¶</a></p>
<p id="section-6-3">The new TCP_REPLENISH_TIME socket option
should have the same semantics across all
operating systems and network stack implementations.<a href="#section-6-3" class="pilcrow">¶</a></p>
<p id="section-6-4">Other transport protocols, like QUIC,
and other network APIs not based on BSD Sockets,
should provide equivalent time-based backlog-management
mechanisms, as appropriate to their API design.<a href="#section-6-4" class="pilcrow">¶</a></p>
<p id="section-6-5">The time-based estimate does not need to be perfectly accurate,
either on the part of the transport protocol estimating how much
time remains before the backlog of unsent data is exhausted,
or on the part of the application estimating how much
time it will need generate its next logical block of data.
If the network data rate increases significantly, or a group of
delayed acknowledgments all arrive together, then the transport
protocol could end up discovering that it has overestimated how
much time remains before the data is exhausted.
If the operating system scheduler is slow to schedule the
application process, or the CPU is busy with other tasks,
then the application may discover that it has
underestimated how much time it will take
to generate its next logical block of data.
These situations are not considered to be serious problems,
especially if they only occur infrequently.
For a delay-sensitive application, having some reasonable
mechanism to avoid an excessive backlog of unsent data is
dramatically better than having no such mechanism at all.
Occasional overestimates or underestimates do not
negate the benefit of this capability.<a href="#section-6-5" class="pilcrow">¶</a></p>
<div id="solicitation-for-name-suggestions">
<section id="section-6.1">
        <h3 id="name-solicitation-for-name-sugge">
<a href="#section-6.1" class="section-number selfRef">6.1. </a><a href="#name-solicitation-for-name-sugge" class="section-name selfRef">Solicitation for Name Suggestions</a>
        </h3>
<p id="section-6.1-1">Author’s note: The BSD socket option name “TCP_REPLENISH_TIME”
is currently proposed as a working name
for this new option for BSD Sockets.
While the name does not affect the behavior of the code,
the choice of name is important, because people often
form their first impressions of a concept based on its name,
and if they form incorrect first impressions then their
thinking about the concept may be adversely affected.<a href="#section-6.1-1" class="pilcrow">¶</a></p>
<p id="section-6.1-2">For example, the BSD socket option could be called
“TCP_REPLENISH_TIME” or “TCP_EXHAUSTION_TIME”.
These are two sides of the same coin.
From the application’s point of view, it is expressing
how much time it will require to replenish the buffer.
From the networking code’s point of view, it is estimating
how much time remains before it will need the buffer replenished.
In an ideal world,
REPLENISH_TIME == EXHAUSTION_TIME, so that the data is
replenished at exactly the moment the networking code needs it.
In a sense, they are two ways of saying the same thing.
Since this API call is made by the application, we feel it
should be expressed in terms of the application’s requirement.<a href="#section-6.1-2" class="pilcrow">¶</a></p>
</section>
</div>
</section>
</div>
<div id="applicability">
<section id="section-7">
      <h2 id="name-applicability">
<a href="#section-7" class="section-number selfRef">7. </a><a href="#name-applicability" class="section-name selfRef">Applicability</a>
      </h2>
<p id="section-7-1">This time-based backlog management is applicable anywhere
that a queue of unsent data may build up on the sending device.<a href="#section-7-1" class="pilcrow">¶</a></p>
<div id="physical-bottlenecks">
<section id="section-7.1">
        <h3 id="name-physical-bottlenecks">
<a href="#section-7.1" class="section-number selfRef">7.1. </a><a href="#name-physical-bottlenecks" class="section-name selfRef">Physical Bottlenecks</a>
        </h3>
<p id="section-7.1-1">A backlog may build up on the sending device if the source
of the packets is simply generating them faster than
the outgoing first-hop interface is able to send them.
This will cause a queue to build up in the network
hardware or its associated driver.
In this case,
to avoid packets suffering excessive queueing delay,
the hardware or its driver
needs to communicate backpressure to IP, which
needs to communicate backpressure to
the transport protocol (TCP or QUIC), which
needs to communicate backpressure to
the application that is the source of the data.
We refer to this case as a physical bottleneck.<a href="#section-7.1-1" class="pilcrow">¶</a></p>
<p id="section-7.1-2">For an example of a physical bottleneck,
consider the case when a user has symmetric
1Gb/s Internet service,
and they are sending data from a device
communicating via Wi-Fi at a lower rate, say 300 Mb/s.
In this case (assuming it is communicating with a
well-connected server on the Internet) the sending
device’s Wi-Fi interface is the limiting factor.
If the Wi-Fi hardware, driver, and networking software
does not produce appropriate backpressure, then outgoing
network traffic will experience increasing delays.<a href="#section-7.1-2" class="pilcrow">¶</a></p>
<p id="section-7.1-3">Poor backpressure from first-hop physical bottlenecks
can produce the ironic outcome that upgrading
home Internet service from 100Mb/s to 1Gb/s can sometimes
result in a customer getting a worse user experience,
because the upgrade causes the bottleneck hop to change location,
from the Internet gateway
(which may have good queue mangement using L4S <span>[<a href="#RFC9330" class="cite xref">RFC9330</a>]</span>)
to the source device’s Wi-Fi interface,
which may have very poor source buffer management.<a href="#section-7.1-3" class="pilcrow">¶</a></p>
</section>
</div>
<div id="algorithmic-bottlenecks">
<section id="section-7.2">
        <h3 id="name-algorithmic-bottlenecks">
<a href="#section-7.2" class="section-number selfRef">7.2. </a><a href="#name-algorithmic-bottlenecks" class="section-name selfRef">Algorithmic Bottlenecks</a>
        </h3>
<p id="section-7.2-1">In addition to physical bottlenecks,
there are other reasons why software on the sending
device may choose to refrain from sending data as fast
as the outgoing first-hop interface can carry it.
We refer to these as algorithmic bottlenecks.<a href="#section-7.2-1" class="pilcrow">¶</a></p>
<p id="section-7.2-2">In the case study in <a href="#casestudy" class="auto internal xref">Section 4</a>, the bottleneck was the
transport protocol’s rate management (congestion control) algorithm,
not a physical constraint of the outgoing first-hop interface
(which was gigabit Ethernet).<a href="#section-7.2-2" class="pilcrow">¶</a></p>
<ul class="normal">
<li class="normal" id="section-7.2-3.1">
            <p id="section-7.2-3.1.1">If the TCP receive window is full, then the sending TCP
implementation will voluntarily refrain from sending new data,
even though the device’s outgoing first-hop interface is easily
capable of sending those packets.
This is vital to avoid overrunning the receiver with data
faster than it can process it.<a href="#section-7.2-3.1.1" class="pilcrow">¶</a></p>
</li>
          <li class="normal" id="section-7.2-3.2">
            <p id="section-7.2-3.2.1">The transport protocol’s rate management (congestion control) algorithm
may determine that it should delay before sending more data, so as
not to overflow a queue at some other bottleneck within the network.
This is vital to avoid overrunning the capacity of the bottleneck
network hop with data faster than it can forward it,
resulting in massive packet loss,
which would equate to a large wastage of resources at the sender,
in the form of battery power and network capacity wasted by
generating packets that will not make it to the receiver.<a href="#section-7.2-3.2.1" class="pilcrow">¶</a></p>
</li>
          <li class="normal" id="section-7.2-3.3">
            <p id="section-7.2-3.3.1">When packet pacing is being used, the sending network
implementation may choose voluntarily to moderate the rate at
which it emits packets, so as to smooth the flow of packets into
the network, even though the device’s outgoing first-hop interface
might be easily capable of sending at a much higher rate.
When packet pacing is being used, a temporary backlog
can build up at this layer if the source is generating
data faster than the pacing rate.<a href="#section-7.2-3.3.1" class="pilcrow">¶</a></p>
</li>
        </ul>
<p id="section-7.2-4">Whether the source application is constrained
by a physical bottleneck on the sending device, or
by an algorithmic bottleneck on the sending device,
the benefits of not overcommitting data to the outgoing buffer are similar.<a href="#section-7.2-4" class="pilcrow">¶</a></p>
<p id="section-7.2-5">As described in the introduction,
the goal is for the application software to be able to
write chunks of data large enough to be efficient,
without writing too many of them too quickly,
and causing unwanted self-inflicted delay.<a href="#section-7.2-5" class="pilcrow">¶</a></p>
</section>
</div>
<div id="superiority-of-direct-backpressure">
<section id="section-7.3">
        <h3 id="name-superiority-of-direct-backp">
<a href="#section-7.3" class="section-number selfRef">7.3. </a><a href="#name-superiority-of-direct-backp" class="section-name selfRef">Superiority of Direct Backpressure</a>
        </h3>
<p id="section-7.3-1">Since multi-hop network protocols already implement
indirect backpressure in the form of discarding or marking packets,
it can be tempting to use the same mechanism
to generate backpressure for first-hop physical bottlenecks.
Superficially there might seem to be some attractive
elegance in having the first hop use the same drop/mark
mechanism as the remaining hops on the path.
However, this is not an ideal solution because indirect
backpressure from the network is very crude compared to
the much richer direct backpressure
that is available within the sending device itself.
Relying on indirect backpressure by
discarding or marking a packet in the sending device itself
is a crude rate-control signal, because it takes a full network
round-trip time before the effect of that drop or mark is
observed at the receiver and echoed back to the sender, and
it may take multiple such round trips before it finally
results in an appropriate reduction in sending rate.<a href="#section-7.3-1" class="pilcrow">¶</a></p>
<p id="section-7.3-2">In contrast to queue buildup in the network,
queue buildup at the sending device has different properties
regarding (i) security, (ii) packet size constraints, and (iii) immediacy.
This means that when it is the source device itself
that is building up a backlog of unsent data,
designers of networking software have more freedom about how to manage this.<a href="#section-7.3-2" class="pilcrow">¶</a></p>
<p id="section-7.3-3">(i) When the source of the data and the location of the backlog are
the same physical device, network security and trust concerns do not apply.<a href="#section-7.3-3" class="pilcrow">¶</a></p>
<p id="section-7.3-4">(ii) When the mechanism we use to communicate about queue state
is a software API instead of packets sent though a network,
we do not have the constraint of having to work within
limited IP packet header space.<a href="#section-7.3-4" class="pilcrow">¶</a></p>
<p id="section-7.3-5">(iii) When flow control is implemented via a local software API,
the delivery of STOP/GO information to the source is immediate.<a href="#section-7.3-5" class="pilcrow">¶</a></p>
<p id="section-7.3-6">Direct backpressure can be achieved
simply making an API call block,
or by returning a Unix EWOULDBLOCK error,
or using equivalent mechanisms in other APIs,
and has the effect of immediately halting the flow of new data.
Similarly, when the system becomes able to accept more data,
unblocking an API call, indicating that a socket
has become writable using select() or kevent(),
or equivalent mechanisms in other APIs,
has the effect of immediately allowing the production of more data.<a href="#section-7.3-6" class="pilcrow">¶</a></p>
<p id="section-7.3-7">Indirect backpressure is vastly inferior to direct backpressure.
For rate adjustment signals generated within the network,
indirect backpressure has to be used because
in that situation better alternatives are not available.
Direct backpressure is vastly superior,
and where direct backpressure mechanisms are possible they
should be preferred over indirect backpressure mechanisms.<a href="#section-7.3-7" class="pilcrow">¶</a></p>
<p id="section-7.3-8">While there might seem to be some attractive elegance
to using indirect backpressure for all hops in the network,
having the first hop be the bottleneck is a common case,
and that is the case where indirect backpressure is at
its worst (it takes an entire network round trip to
learn what is already known on the sending device),
so the benefits mean that it is worth
optimizing for this common case and making use
of direct backpressure for first hop bottlenecks.<a href="#section-7.3-8" class="pilcrow">¶</a></p>
</section>
</div>
<div id="application-programming-interface">
<section id="section-7.4">
        <h3 id="name-application-programming-int">
<a href="#section-7.4" class="section-number selfRef">7.4. </a><a href="#name-application-programming-int" class="section-name selfRef">Application Programming Interface</a>
        </h3>
<p id="section-7.4-1">It is important to understand that these
backpressure mechanisms at the API layer are not new.
They have existed by necessity for as long as we have had
networking APIs (or serial port APIs, or file system APIs, etc.).
The problem with these APIs has been that historically
these backpressure mechanisms were exercised too late,
after an excessive backlog had already built up.<a href="#section-7.4-1" class="pilcrow">¶</a></p>
<p id="section-7.4-2">The proposal in this Source Buffer Management
document is not to define entirely new API mechanisms
that did not previously exist, or to fundamentally
change how networking applications are written;
the proposal is to use existing
networking API mechanisms more effectively.
Depending on how a networking application is written,
using kevent() or similar mechanisms
to tell it when it is time to write to a socket,
it may be that the only change the application
needs is a single call using TCP_REPLENISH_TIME to indicate
its expected time budget to generate a new block
of data, and everything else in the application
remains completely unchanged.<a href="#section-7.4-2" class="pilcrow">¶</a></p>
</section>
</div>
<div id="relationship-between-throughput-and-delay">
<section id="section-7.5">
        <h3 id="name-relationship-between-throug">
<a href="#section-7.5" class="section-number selfRef">7.5. </a><a href="#name-relationship-between-throug" class="section-name selfRef">Relationship Between Throughput and Delay</a>
        </h3>
<p id="section-7.5-1">Is is important to understand that Source Buffer Management
using TCP_REPLENISH_TIME does not alter the overall
long-term average throughput of a data transfer.
Calculating the optimum rate to send data
(so as not to exceed receiver’s capacity,
or the available network capacity)
remains the responsibility of the transport protocol.
Using TCP_REPLENISH_TIME does not alter the data rate;
it controls the delay between the time when data is generated
and the time when that data departs the sending device.
Using the example from <a href="#casestudy" class="auto internal xref">Section 4</a>, in both cases
the long-term average throughput was 500 kb/s.
What changed was that originally the application was
generating 500 kb/s with two seconds of outgoing delay;
after using TCP_REPLENISH_TIME the application was
generating 500 kb/s with 250 milliseconds of outgoing delay.<a href="#section-7.5-1" class="pilcrow">¶</a></p>
</section>
</div>
<div id="bulk-transfer-protocols">
<section id="section-7.6">
        <h3 id="name-bulk-transfer-protocols">
<a href="#section-7.6" class="section-number selfRef">7.6. </a><a href="#name-bulk-transfer-protocols" class="section-name selfRef">Bulk Transfer Protocols</a>
        </h3>
<p id="section-7.6-1">It is frequently asserted that latency matters primarily for
interactive applications like video conferencing and on-line games,
and latency is relatively unimportant for most other applications.<a href="#section-7.6-1" class="pilcrow">¶</a></p>
<p id="section-7.6-2">We do not agree with this characterization.<a href="#section-7.6-2" class="pilcrow">¶</a></p>
<p id="section-7.6-3">Even for large bulk data transfers
-- e.g., downloading a software update or uploading a video --
we believe latency affects performance.<a href="#section-7.6-3" class="pilcrow">¶</a></p>
<p id="section-7.6-4">For example, TCP fast retransmit can immediately
recover a single lost packet in a single round-trip time.
TCP generally performs at its absolute best when the
loss rate is no more than one loss per round-trip time.
More than one loss per round-trip time requires more
extensive use of TCP SACK blocks, which consume extra
space in the packet header, and makes the work of the
rate management (congestion control) algorithm harder.
This can result in the transport protocol temporarily
sending too fast, resulting in additional packet loss,
or too slowly, resulting in underutilized network capacity.
For a given fixed loss rate (in packets lost per second)
a higher total network round-trip time
(including the time spent in buffers in the sending network
interface, below the transport protocol layer)
equates to more lost packets per network round-trip time,
causing error recovery to occur less quickly.
A transport protocol cannot make rate adaptation changes
to adjust to varying network conditions in less than one
network round-trip time, so the higher the total network
round-trip time is, the less agile the transport protocol
is at adjusting to varying network conditions.<a href="#section-7.6-4" class="pilcrow">¶</a></p>
<p id="section-7.6-5">In short, a client running over a transport protocol like TCP
may itself not be a real-time delay-sensitive application,
but a transport protocol itself is most definitely a
delay-sensitive application, responding in real time
to changing network conditions.<a href="#section-7.6-5" class="pilcrow">¶</a></p>
</section>
</div>
</section>
</div>
<div id="alternative-proposals">
<section id="section-8">
      <h2 id="name-alternative-proposals">
<a href="#section-8" class="section-number selfRef">8. </a><a href="#name-alternative-proposals" class="section-name selfRef">Alternative Proposals</a>
      </h2>
<div id="just-use-udp">
<section id="section-8.1">
        <h3 id="name-just-use-udp">
<a href="#section-8.1" class="section-number selfRef">8.1. </a><a href="#name-just-use-udp" class="section-name selfRef">Just use UDP</a>
        </h3>
<p id="section-8.1-1">Because much of the discussion about network latency involves
talking about the behavior of transport protocols like TCP,
sometimes people conclude that TCP is the problem,
and think that using UDP will solve the source buffering problem.
It does no such thing.
If an application sends UDP packets faster than the outgoing
network interface can carry them, then a queue of packets
will still build up, causing increasing delay for those packets,
and eventual packet loss when the queue reaches its capacity.<a href="#section-8.1-1" class="pilcrow">¶</a></p>
<p id="section-8.1-2">Any protocol that runs over UDP
(like QUIC <span>[<a href="#RFC9000" class="cite xref">RFC9000</a>]</span><span>[<a href="#RFC9369" class="cite xref">RFC9369</a>]</span>) must end up
re-creating the same rate optimization behaviors that
are already built into TCP, or it will fail to operate
gracefully over a range of different network conditions.<a href="#section-8.1-2" class="pilcrow">¶</a></p>
</section>
</div>
<div id="packet-expiration">
<section id="section-8.2">
        <h3 id="name-packet-expiration">
<a href="#section-8.2" class="section-number selfRef">8.2. </a><a href="#name-packet-expiration" class="section-name selfRef">Packet Expiration</a>
        </h3>
<p id="section-8.2-1">One approach that is sometimes used, is to send packets
tagged with an expiration time, and if they have spent
too long waiting in the outgoing queue then they are
automatically discarded without even being sent.
This is counterproductive because the sending application
does all the work to generate data, and then has to do more
work to recover from the self-inflicted data loss caused by
the expiration time.<a href="#section-8.2-1" class="pilcrow">¶</a></p>
<p id="section-8.2-2">If the outgoing queue is kept short, then the
amount of unwanted delay is kept correspondingly short.
In addition, if there is only a small amount of data in the
outgoing queue, then the cost of sending a small amount of
data that may arguably have become stale is also small --
usually smaller than the cost of having to recover missing
state caused by intentional discard of that delayed data.<a href="#section-8.2-2" class="pilcrow">¶</a></p>
<p id="section-8.2-3">For example, in video conferencing applications it is
frequently thought that if a frame is delayed past the
point where it becomes too late to display it, then it becomes
a waste of network capacity to send that frame at all.
However, the fallacy in that argument is that modern
video compression algorithms make extensive use of
similarity between consecutive frames.
A given video frame is not just encoded as a single frame
in isolation, but as a collection of visual
differences relative to the previous frame.
The previous frame may have arrived too late for the
time it was supposed to be displayed, but the data
contained within it is still needed to decode and
display the current frame.
If the previous frame was intentionally discarded by the
sender, then the subsequent frames are also impacted by
that loss, and the cost of repairing the damage is
frequently much higher than the cost would have been
to simply send the delayed frame.
Just because a frame arrives too late to be displayed does
not mean that the data within that frame is not important.
The data contained with a frame is used not only to display
that frame, but also in the construction of subsequent frames.<a href="#section-8.2-3" class="pilcrow">¶</a></p>
</section>
</div>
<div id="head-of-line-blocking-traffic-priorities">
<section id="section-8.3">
        <h3 id="name-head-of-line-blocking-traff">
<a href="#section-8.3" class="section-number selfRef">8.3. </a><a href="#name-head-of-line-blocking-traff" class="section-name selfRef">Head of Line Blocking / Traffic Priorities</a>
        </h3>
<p id="section-8.3-1">People are often very concerned about the problem of
head-of-line-blocking, and propose to solve it using
techniques such as packet priorities,
and the ability cancel unsent pending messages <span>[<a href="#MMADAPT" class="cite xref">MMADAPT</a>]</span>.
There is an unconscious unstated assumption baked into
this line of reasoning, which is that having an excessively
long queue is inevitable and unavoidable, and therefore
we have to devote a lot of our energy into how to organize
and prioritize and manage that obligatory excessive queue.
In contrast, if we take steps to keep queues short,
the problems head-of-line-blocking largely go away.
When the line is consistently short, being at the back of
the line is no longer the serious problem that it used to be.<a href="#section-8.3-1" class="pilcrow">¶</a></p>
</section>
</div>
</section>
</div>
<div id="security-considerations">
<section id="section-9">
      <h2 id="name-security-considerations">
<a href="#section-9" class="section-number selfRef">9. </a><a href="#name-security-considerations" class="section-name selfRef">Security Considerations</a>
      </h2>
<p id="section-9-1">No security concerns are anticipated resulting from reducing
the amount of stale data sitting in buffers at the sender.<a href="#section-9-1" class="pilcrow">¶</a></p>
</section>
</div>
<div id="iana-considerations">
<section id="section-10">
      <h2 id="name-iana-considerations">
<a href="#section-10" class="section-number selfRef">10. </a><a href="#name-iana-considerations" class="section-name selfRef">IANA Considerations</a>
      </h2>
<p id="section-10-1">This document has no IANA actions.<a href="#section-10-1" class="pilcrow">¶</a></p>
</section>
</div>
<div id="sec-combined-references">
<section id="section-11">
      <h2 id="name-references">
<a href="#section-11" class="section-number selfRef">11. </a><a href="#name-references" class="section-name selfRef">References</a>
      </h2>
<div id="sec-normative-references">
<section id="section-11.1">
        <h3 id="name-normative-references">
<a href="#section-11.1" class="section-number selfRef">11.1. </a><a href="#name-normative-references" class="section-name selfRef">Normative References</a>
        </h3>
<dl class="references">
<dt id="RFC2119">[RFC2119]</dt>
        <dd>
<span class="refAuthor">Bradner, S.</span>, <span class="refTitle">"Key words for use in RFCs to Indicate Requirement Levels"</span>, <span class="seriesInfo">BCP 14</span>, <span class="seriesInfo">RFC 2119</span>, <span class="seriesInfo">DOI 10.17487/RFC2119</span>, <time datetime="1997-03" class="refDate">March 1997</time>, <span>&lt;<a href="https://www.rfc-editor.org/rfc/rfc2119">https://www.rfc-editor.org/rfc/rfc2119</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC8174">[RFC8174]</dt>
      <dd>
<span class="refAuthor">Leiba, B.</span>, <span class="refTitle">"Ambiguity of Uppercase vs Lowercase in RFC 2119 Key Words"</span>, <span class="seriesInfo">BCP 14</span>, <span class="seriesInfo">RFC 8174</span>, <span class="seriesInfo">DOI 10.17487/RFC8174</span>, <time datetime="2017-05" class="refDate">May 2017</time>, <span>&lt;<a href="https://www.rfc-editor.org/rfc/rfc8174">https://www.rfc-editor.org/rfc/rfc8174</a>&gt;</span>. </dd>
<dd class="break"></dd>
</dl>
</section>
</div>
<div id="sec-informative-references">
<section id="section-11.2">
        <h3 id="name-informative-references">
<a href="#section-11.2" class="section-number selfRef">11.2. </a><a href="#name-informative-references" class="section-name selfRef">Informative References</a>
        </h3>
<dl class="references">
<dt id="Bloat1">[Bloat1]</dt>
        <dd>
<span class="refAuthor">Gettys, J.</span>, <span class="refTitle">"Whose house is of glasse, must not throw stones at another"</span>, <time datetime="2010-12" class="refDate">December 2010</time>, <span>&lt;<a href="https://gettys.wordpress.com/2010/12/06/whose-house-is-of-glasse-must-not-throw-stones-at-another/">https://gettys.wordpress.com/2010/12/06/whose-house-is-of-glasse-must-not-throw-stones-at-another/</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="Bloat2">[Bloat2]</dt>
        <dd>
<span class="refAuthor">Gettys, J.</span> and <span class="refAuthor">K. Nichols</span>, <span class="refTitle">"Bufferbloat: Dark Buffers in the Internet"</span>, <span class="seriesInfo">ACM Queue, Volume 9, issue 11 </span>, <time datetime="2011-11" class="refDate">November 2011</time>, <span>&lt;<a href="https://queue.acm.org/detail.cfm?id=2071893">https://queue.acm.org/detail.cfm?id=2071893</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="Bloat3">[Bloat3]</dt>
        <dd>
<span class="refAuthor">Gettys, J.</span> and <span class="refAuthor">K. Nichols</span>, <span class="refTitle">"Bufferbloat: Dark Buffers in the Internet"</span>, <span class="seriesInfo">Communications of the ACM, Volume 55, Number 1 </span>, <time datetime="2012-01" class="refDate">January 2012</time>, <span>&lt;<a href="https://dl.acm.org/doi/10.1145/2063176.2063196">https://dl.acm.org/doi/10.1145/2063176.2063196</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="Cake">[Cake]</dt>
        <dd>
<span class="refAuthor">Høiland-Jørgensen, T.</span>, <span class="refAuthor">Taht, D.</span>, and <span class="refAuthor">J. Morton</span>, <span class="refTitle">"Piece of CAKE: A Comprehensive Queue Management Solution for Home Gateways"</span>, <span class="seriesInfo">2018 IEEE International Symposium on Local and Metropolitan Area Networks (LANMAN) </span>, <time datetime="2018-06" class="refDate">June 2018</time>, <span>&lt;<a href="https://ieeexplore.ieee.org/document/8475045">https://ieeexplore.ieee.org/document/8475045</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="Demo">[Demo]</dt>
        <dd>
<span class="refAuthor">Cheshire, S.</span>, <span class="refTitle">"Your App and Next Generation Networks"</span>, <span class="seriesInfo">Apple Worldwide Developer Conference </span>, <time datetime="2015-06" class="refDate">June 2015</time>, <span>&lt;<a href="https://developer.apple.com/videos/play/wwdc2015/719/?time=2199">https://developer.apple.com/videos/play/wwdc2015/719/?time=2199</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="MMADAPT">[MMADAPT]</dt>
        <dd>
<span class="refAuthor">Aiman Erbad</span> and <span class="refAuthor">Charles Buck Krasic</span>, <span class="refTitle">"Sender-side buffers and the case for multimedia adaptation"</span>, <span class="seriesInfo">ACM Queue, Volume 10, issue 10 </span>, <time datetime="2012-10" class="refDate">October 2012</time>, <span>&lt;<a href="https://queue.acm.org/detail.cfm?id=2381998">https://queue.acm.org/detail.cfm?id=2381998</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC3168">[RFC3168]</dt>
        <dd>
<span class="refAuthor">Ramakrishnan, K.</span>, <span class="refAuthor">Floyd, S.</span>, and <span class="refAuthor">D. Black</span>, <span class="refTitle">"The Addition of Explicit Congestion Notification (ECN) to IP"</span>, <span class="seriesInfo">RFC 3168</span>, <span class="seriesInfo">DOI 10.17487/RFC3168</span>, <time datetime="2001-09" class="refDate">September 2001</time>, <span>&lt;<a href="https://www.rfc-editor.org/rfc/rfc3168">https://www.rfc-editor.org/rfc/rfc3168</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC6143">[RFC6143]</dt>
        <dd>
<span class="refAuthor">Richardson, T.</span> and <span class="refAuthor">J. Levine</span>, <span class="refTitle">"The Remote Framebuffer Protocol"</span>, <span class="seriesInfo">RFC 6143</span>, <span class="seriesInfo">DOI 10.17487/RFC6143</span>, <time datetime="2011-03" class="refDate">March 2011</time>, <span>&lt;<a href="https://www.rfc-editor.org/rfc/rfc6143">https://www.rfc-editor.org/rfc/rfc6143</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC8033">[RFC8033]</dt>
        <dd>
<span class="refAuthor">Pan, R.</span>, <span class="refAuthor">Natarajan, P.</span>, <span class="refAuthor">Baker, F.</span>, and <span class="refAuthor">G. White</span>, <span class="refTitle">"Proportional Integral Controller Enhanced (PIE): A Lightweight Control Scheme to Address the Bufferbloat Problem"</span>, <span class="seriesInfo">RFC 8033</span>, <span class="seriesInfo">DOI 10.17487/RFC8033</span>, <time datetime="2017-02" class="refDate">February 2017</time>, <span>&lt;<a href="https://www.rfc-editor.org/rfc/rfc8033">https://www.rfc-editor.org/rfc/rfc8033</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC8290">[RFC8290]</dt>
        <dd>
<span class="refAuthor">Hoeiland-Joergensen, T.</span>, <span class="refAuthor">McKenney, P.</span>, <span class="refAuthor">Taht, D.</span>, <span class="refAuthor">Gettys, J.</span>, and <span class="refAuthor">E. Dumazet</span>, <span class="refTitle">"The Flow Queue CoDel Packet Scheduler and Active Queue Management Algorithm"</span>, <span class="seriesInfo">RFC 8290</span>, <span class="seriesInfo">DOI 10.17487/RFC8290</span>, <time datetime="2018-01" class="refDate">January 2018</time>, <span>&lt;<a href="https://www.rfc-editor.org/rfc/rfc8290">https://www.rfc-editor.org/rfc/rfc8290</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC9000">[RFC9000]</dt>
        <dd>
<span class="refAuthor">Iyengar, J., Ed.</span> and <span class="refAuthor">M. Thomson, Ed.</span>, <span class="refTitle">"QUIC: A UDP-Based Multiplexed and Secure Transport"</span>, <span class="seriesInfo">RFC 9000</span>, <span class="seriesInfo">DOI 10.17487/RFC9000</span>, <time datetime="2021-05" class="refDate">May 2021</time>, <span>&lt;<a href="https://www.rfc-editor.org/rfc/rfc9000">https://www.rfc-editor.org/rfc/rfc9000</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC9330">[RFC9330]</dt>
        <dd>
<span class="refAuthor">Briscoe, B., Ed.</span>, <span class="refAuthor">De Schepper, K.</span>, <span class="refAuthor">Bagnulo, M.</span>, and <span class="refAuthor">G. White</span>, <span class="refTitle">"Low Latency, Low Loss, and Scalable Throughput (L4S) Internet Service: Architecture"</span>, <span class="seriesInfo">RFC 9330</span>, <span class="seriesInfo">DOI 10.17487/RFC9330</span>, <time datetime="2023-01" class="refDate">January 2023</time>, <span>&lt;<a href="https://www.rfc-editor.org/rfc/rfc9330">https://www.rfc-editor.org/rfc/rfc9330</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC9369">[RFC9369]</dt>
      <dd>
<span class="refAuthor">Duke, M.</span>, <span class="refTitle">"QUIC Version 2"</span>, <span class="seriesInfo">RFC 9369</span>, <span class="seriesInfo">DOI 10.17487/RFC9369</span>, <time datetime="2023-05" class="refDate">May 2023</time>, <span>&lt;<a href="https://www.rfc-editor.org/rfc/rfc9369">https://www.rfc-editor.org/rfc/rfc9369</a>&gt;</span>. </dd>
<dd class="break"></dd>
</dl>
</section>
</div>
</section>
</div>
<div id="acknowledgments">
<section id="appendix-A">
      <h2 id="name-acknowledgments">
<a href="#name-acknowledgments" class="section-name selfRef">Acknowledgments</a>
      </h2>
<p id="appendix-A-1">This document has benefited from input and suggestions from:
Chris Box,
Morten Brørup,
Neal Cardwell,
Yuchung Cheng,
Eric Dumazet,
Sebastian Moeller,
Christoph Paasch,
Kevin Smith,
Ian Swett,
all who joined the side meeting at IETF 121 in Dublin (November 2024),
and others [please don’t be shy about reminding me if I somehow missed your name].<a href="#appendix-A-1" class="pilcrow">¶</a></p>
</section>
</div>
<div id="authors-addresses">
<section id="appendix-B">
      <h2 id="name-authors-address">
<a href="#name-authors-address" class="section-name selfRef">Author's Address</a>
      </h2>
<address class="vcard">
        <div dir="auto" class="left"><span class="fn nameRole">Stuart Cheshire</span></div>
<div dir="auto" class="left"><span class="org">Apple Inc.</span></div>
<div class="email">
<span>Email:</span>
<a href="mailto:cheshire@apple.com" class="email">cheshire@apple.com</a>
</div>
</address>
</section>
</div>
<script>const toc = document.getElementById("toc");
toc.querySelector("h2").addEventListener("click", e => {
  toc.classList.toggle("active");
});
toc.querySelector("nav").addEventListener("click", e => {
  toc.classList.remove("active");
});
</script>
</body>
</html>
